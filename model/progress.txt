WHAT I HAVE DONE

#MLRT	#	5	#ML	LRT	False	False	#MLRT	IV
token 	pos 	length	prefix	suffix	hasrepconchar	isAlpha	normalized form OOV|IV

1. From training set, extracted features
	FEATURE GENERATION
		a. token
		b. pos tag
		c. OOV - if token is not in the SCOWL dictionary
		   IV - if token is in the SCOWL dictionary

		   SCOWL -> edited to accept only A(a) and I rather than all the alphabets (Upper|Lower)
		d. token before the current token
		e. token after the current token
		f. pos tag of token before the current token
		g. pos tag of token after the current token
		h. proper nouns, usernames, mentions, punctuations, numerals and hashtags are considered IV
		i. word length
		j. numeric tokens are considered as IV
		k. prefix and suffix of the token (benchmark) [3 characters before and after]
		l. repetitive characters
		m. alphanumeric tokens - high probability of being OOV
		n. normalized form of the token [to be extracted from the training set]

		To be implemented features: 
		o. edit distance feature
		p. check if token is in the lexical normalization dictionary
		   OOV - if token is in the lexical normalization dictionary
		   IV - if token is in not the lexical normalization dictionary

2. Crawled Sunstar's first page using scrapy

WHAT TO DO NEXT

1. [x] have a recursive crawler that crawls all news in the sunstar website
2. [x] clean crawled data by using regex to remove all html tags
	- "\n" -> translate to new line
	- "\u" -> remove all unnecessary tags
3. set up word2vec model
4. feed data to word2vec model

=======================================================================================

1. Since the CRF model has been trained to detect OOV tokens, the next step is to:
	- check if OOV token is in normalization lexical dictionary:
		= if token is in the dictionary -> get the normalized form
		= else -> use GOUWS methods to normalize the OOV token

2. Expand dictionary to accomodate not only english words but also tagalog and bisaya

=======================================================================================

GIT

* git add -A
* git commit -m "message"
* git push -u SP master

For crawling

scrapy crawl spider -o items.csv -t csv
python cleanlinks.py
scrapy crawl body
python para.py
python cleansentences.py

citing gensim:
@inproceedings{rehurek_lrec,
      title = {{Software Framework for Topic Modelling with Large Corpora}},
      author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
      booktitle = {{Proceedings of the LREC 2010 Workshop on New
           Challenges for NLP Frameworks}},
      pages = {45--50},
      year = 2010,
      month = May,
      day = 22,
      publisher = {ELRA},
      address = {Valletta, Malta},
      note={\url{http://is.muni.cz/publication/884893/en}},
      language={English}
}

GENSIM

# min_count=10 
# [Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage]
# size=200
# NN layers means “degrees” of freedom the training algorithm has
# window=5